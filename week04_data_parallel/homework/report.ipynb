{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework report (Roma Nigmatullin)\n",
    "##### Changes\n",
    "- Training run with custom flag from script, how to run: copy commands from jupyter.\n",
    "- Added memory allocation / reservation metrics, training time metrics (total and per epoch).\n",
    "- Added validation accuracy metrics, aggregated over all processes via all_reduce of both numerator and denominator of ratio metric.\n",
    "\n",
    "##### Source code\n",
    "SyncBatchNorm is in `syncbn.py`, tests are in `test_syncbn.py`.\n",
    "\n",
    "Pipelines are in `ddp_cifar100.py` and `ddp_cifar100_benchmark.py` with all the changes\n",
    "- `ddp_cifar100.py` contains dataset creation and dataloader creation, model class, metric reducing functions.\n",
    "- `ddp_cifar100_benchmark.py` contains two pipelines with training loop and validation, and the runner script with --custom flag.\n",
    "\n",
    "Benchmarking of implementations of SyncBatchNorm is in `syncbn_benchmark.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytest==8.3.4\n",
      "torch==2.4.0\n",
      "torchvision==0.19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.9\n"
     ]
    }
   ],
   "source": [
    "# run from week04_data_parallel/homework directory, uv project initialized\n",
    "# requirements.txt contains all the dependencies\n",
    "!cat requirements.txt\n",
    "!echo ''\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 23 13:42:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000                On  |   00000000:02:00.0 Off |                  N/A |\n",
      "| 30%   29C    P8             16W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:05:00.0 Off |                  Off |\n",
      "| 41%   37C    P8             16W /  140W |    6477MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:06:00.0 Off |                  Off |\n",
      "| 41%   33C    P8             14W /  140W |       4MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Quadro RTX 4000                On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8              7W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Quadro RTX 4000                On  |   00000000:82:00.0 Off |                  N/A |\n",
      "| 30%   25C    P8             10W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  Quadro RTX 4000                On  |   00000000:85:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8              9W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  Quadro RTX 4000                On  |   00000000:86:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8              9W /  125W |       4MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A   3792563      C   /usr/bin/python3                              158MiB |\n",
      "|    1   N/A  N/A   4044607      C   ...ovaza/anaconda3/envs/sfe/bin/python       6304MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# used GPUs - NVIDIA Quadro RTX 4000 (x2) and 1 process per GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/nigmatullinro/efficient-dl-systems/week04_data_parallel/homework\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-4.8.0\n",
      "collected 16 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_syncbn.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 63.03s (0:01:03)\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv run pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running and benchmarking training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0223 13:20:48.095000 140558715837312 torch/distributed/run.py:779] \n",
      "W0223 13:20:48.095000 140558715837312 torch/distributed/run.py:779] *****************************************\n",
      "W0223 13:20:48.095000 140558715837312 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0223 13:20:48.095000 140558715837312 torch/distributed/run.py:779] *****************************************\n",
      "Epoch 0 | train_loss: 4.25703, val_loss: 3.89881, accuracy: 0.15670\n",
      "Epoch 1 | train_loss: 3.84836, val_loss: 3.63068, accuracy: 0.20520\n",
      "Epoch 2 | train_loss: 3.62297, val_loss: 3.45377, accuracy: 0.23890\n",
      "Epoch 3 | train_loss: 3.44466, val_loss: 3.29486, accuracy: 0.26210\n",
      "Epoch 4 | train_loss: 3.28940, val_loss: 3.15547, accuracy: 0.28300\n",
      "Epoch 5 | train_loss: 3.15712, val_loss: 3.03495, accuracy: 0.29630\n",
      "Epoch 6 | train_loss: 3.04107, val_loss: 2.93417, accuracy: 0.31510\n",
      "Epoch 7 | train_loss: 2.94708, val_loss: 2.85057, accuracy: 0.32990\n",
      "Epoch 8 | train_loss: 2.85740, val_loss: 2.79026, accuracy: 0.33390\n",
      "Epoch 9 | train_loss: 2.78531, val_loss: 2.77148, accuracy: 0.33650\n",
      "Epoch 10 | train_loss: 2.71639, val_loss: 2.70234, accuracy: 0.35130\n",
      "Epoch 11 | train_loss: 2.66428, val_loss: 2.65783, accuracy: 0.35770\n",
      "Epoch 12 | train_loss: 2.61325, val_loss: 2.63963, accuracy: 0.35390\n",
      "Epoch 13 | train_loss: 2.55391, val_loss: 2.59789, accuracy: 0.36900\n",
      "Epoch 14 | train_loss: 2.50795, val_loss: 2.58313, accuracy: 0.36490\n",
      "Epoch 15 | train_loss: 2.46556, val_loss: 2.56528, accuracy: 0.36870\n",
      "Epoch 16 | train_loss: 2.42951, val_loss: 2.51918, accuracy: 0.37640\n",
      "Epoch 17 | train_loss: 2.38601, val_loss: 2.52685, accuracy: 0.37650\n",
      "Epoch 18 | train_loss: 2.35019, val_loss: 2.52086, accuracy: 0.38040\n",
      "Epoch 19 | train_loss: 2.31575, val_loss: 2.51569, accuracy: 0.37910\n",
      "Epoch 20 | train_loss: 2.29056, val_loss: 2.50086, accuracy: 0.37600\n",
      "Epoch 21 | train_loss: 2.25068, val_loss: 2.48849, accuracy: 0.38340\n",
      "Epoch 22 | train_loss: 2.22718, val_loss: 2.46202, accuracy: 0.38390\n",
      "Epoch 23 | train_loss: 2.20141, val_loss: 2.46698, accuracy: 0.38370\n",
      "Epoch 24 | train_loss: 2.17111, val_loss: 2.44341, accuracy: 0.38620\n",
      "Epoch 25 | train_loss: 2.13947, val_loss: 2.43778, accuracy: 0.39130\n",
      "Epoch 26 | train_loss: 2.12549, val_loss: 2.42659, accuracy: 0.39220\n",
      "Epoch 27 | train_loss: 2.10252, val_loss: 2.43350, accuracy: 0.38870\n",
      "Epoch 28 | train_loss: 2.07366, val_loss: 2.41650, accuracy: 0.39400\n",
      "Epoch 29 | train_loss: 2.04164, val_loss: 2.41267, accuracy: 0.39420\n",
      "Final validation accuracy: 0.39420\n",
      "Memory allocated in process 0: 184441344\n",
      "Memory allocated in process 1: 152530944Memory reserved in process 0: 195035136\n",
      "\n",
      "Training time in process 0: 332.63 seconds\n",
      "Training time per epoch in process 0: 11.09 seconds\n",
      "Memory reserved in process 1: 220200960\n",
      "Training time in process 1: 332.63 seconds\n",
      "Training time per epoch in process 1: 11.09 seconds\n",
      "[rank0]:[W223 13:26:33.325650870 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5 uv run torchrun --nproc_per_node 2 ddp_cifar100_benchmark.py --custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prettified\n",
    "| Metric    | Process 0 | Process 1 |\n",
    "| -------- | ------- | ------- |\n",
    "| Max memory allocated (CUDA) | 184 MB | 152 MB |\n",
    "| Max memory reserved (CUDA) | 195 MB | 220 MB |\n",
    "| Training time (total) | 332 (s) | 332 (s) |\n",
    "| Training time (per epoch) | 11 (s) | 11 (s) |\n",
    "| Validation accuracy | 0.3942 | 0.3942 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0223 13:26:37.667000 140640212700032 torch/distributed/run.py:779] \n",
      "W0223 13:26:37.667000 140640212700032 torch/distributed/run.py:779] *****************************************\n",
      "W0223 13:26:37.667000 140640212700032 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0223 13:26:37.667000 140640212700032 torch/distributed/run.py:779] *****************************************\n",
      "Epoch 0 | train_loss: 0.00000, val_loss: 3.90105, accuracy: 0.15850\n",
      "Epoch 1 | train_loss: 0.00000, val_loss: 3.63479, accuracy: 0.20360\n",
      "Epoch 2 | train_loss: 0.00000, val_loss: 3.46039, accuracy: 0.23830\n",
      "Epoch 3 | train_loss: 0.00000, val_loss: 3.29393, accuracy: 0.26520\n",
      "Epoch 4 | train_loss: 0.00000, val_loss: 3.16485, accuracy: 0.28240\n",
      "Epoch 5 | train_loss: 0.00000, val_loss: 3.05015, accuracy: 0.29530\n",
      "Epoch 6 | train_loss: 0.00000, val_loss: 2.93685, accuracy: 0.31800\n",
      "Epoch 7 | train_loss: 0.00000, val_loss: 2.86284, accuracy: 0.32790\n",
      "Epoch 8 | train_loss: 0.00000, val_loss: 2.79696, accuracy: 0.33150\n",
      "Epoch 9 | train_loss: 0.00000, val_loss: 2.77059, accuracy: 0.33710\n",
      "Epoch 10 | train_loss: 0.00000, val_loss: 2.70289, accuracy: 0.35740\n",
      "Epoch 11 | train_loss: 0.00000, val_loss: 2.66328, accuracy: 0.35630\n",
      "Epoch 12 | train_loss: 0.00000, val_loss: 2.64044, accuracy: 0.35650\n",
      "Epoch 13 | train_loss: 0.00000, val_loss: 2.60621, accuracy: 0.36770\n",
      "Epoch 14 | train_loss: 0.00000, val_loss: 2.59579, accuracy: 0.36760\n",
      "Epoch 15 | train_loss: 0.00000, val_loss: 2.56982, accuracy: 0.37450\n",
      "Epoch 16 | train_loss: 0.00000, val_loss: 2.51520, accuracy: 0.37990\n",
      "Epoch 17 | train_loss: 0.00000, val_loss: 2.54241, accuracy: 0.37250\n",
      "Epoch 18 | train_loss: 0.00000, val_loss: 2.53856, accuracy: 0.37600\n",
      "Epoch 19 | train_loss: 0.00000, val_loss: 2.50886, accuracy: 0.38250\n",
      "Epoch 20 | train_loss: 0.00000, val_loss: 2.47811, accuracy: 0.38560\n",
      "Epoch 21 | train_loss: 0.00000, val_loss: 2.50287, accuracy: 0.38230\n",
      "Epoch 22 | train_loss: 0.00000, val_loss: 2.45885, accuracy: 0.38840\n",
      "Epoch 23 | train_loss: 0.00000, val_loss: 2.47130, accuracy: 0.38280\n",
      "Epoch 24 | train_loss: 0.00000, val_loss: 2.43946, accuracy: 0.38930\n",
      "Epoch 25 | train_loss: 0.00000, val_loss: 2.44713, accuracy: 0.39000\n",
      "Epoch 26 | train_loss: 0.00000, val_loss: 2.43119, accuracy: 0.39370\n",
      "Epoch 27 | train_loss: 0.00000, val_loss: 2.43774, accuracy: 0.38760\n",
      "Epoch 28 | train_loss: 0.00000, val_loss: 2.41768, accuracy: 0.39550\n",
      "Epoch 29 | train_loss: 0.00000, val_loss: 2.41260, accuracy: 0.39530\n",
      "Final validation accuracy: 0.39530\n",
      "Memory allocated in process 0: 184441344\n",
      "Memory allocated in process 1: 156723712\n",
      "Memory reserved in process 0: 195035136\n",
      "Training time in process 0: 272.26 seconds\n",
      "Training time per epoch in process 0: 9.08 seconds\n",
      "Memory reserved in process 1: 197132288\n",
      "Training time in process 1: 272.27 seconds\n",
      "Training time per epoch in process 1: 9.08 seconds\n",
      "[rank0]:[W223 13:31:19.716566797 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5 uv run torchrun --nproc_per_node 2 ddp_cifar100_benchmark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prettified\n",
    "| Metric    | Process 0 | Process 1 |\n",
    "| -------- | ------- | ------- |\n",
    "| Max memory allocated (CUDA) | 184 MB | 156 MB |\n",
    "| Max memory reserved (CUDA) | 195 MB | 197 MB |\n",
    "| Training time (total) | 272 (s) | 272 (s) |\n",
    "| Training time (per epoch) | 9 (s) | 9 (s) |\n",
    "| Validation accuracy | 0.3953 | 0.3953 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loosing on time, but almost same on memory and accuracy, actually not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking implementations of SyncBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0223 14:28:36.115000 139821546109824 torch/distributed/run.py:779] \n",
      "W0223 14:28:36.115000 139821546109824 torch/distributed/run.py:779] *****************************************\n",
      "W0223 14:28:36.115000 139821546109824 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0223 14:28:36.115000 139821546109824 torch/distributed/run.py:779] *****************************************\n",
      "Rank 0\n",
      "hidden_dim: 128, batch_size: 32\n",
      "Custom SyncBN: 1.2753 (ms / ep), 2.0972 MB reserved, 0.0568 MB allocated\n",
      "Torch SyncBN: 1.0504 (ms / ep), 2.0972 MB reserved, 0.0435 MB allocated\n",
      "\n",
      "hidden_dim: 128, batch_size: 64\n",
      "Custom SyncBN: 1.2660 (ms / ep), 2.0972 MB reserved, 0.1060 MB allocated\n",
      "Torch SyncBN: 1.0681 (ms / ep), 2.0972 MB reserved, 0.0763 MB allocated\n",
      "\n",
      "hidden_dim: 256, batch_size: 32\n",
      "Custom SyncBN: 1.2758 (ms / ep), 2.0972 MB reserved, 0.1126 MB allocated\n",
      "Torch SyncBN: 1.0723 (ms / ep), 2.0972 MB reserved, 0.0835 MB allocated\n",
      "\n",
      "hidden_dim: 256, batch_size: 64\n",
      "Custom SyncBN: 1.3515 (ms / ep), 2.0972 MB reserved, 0.2109 MB allocated\n",
      "Torch SyncBN: 1.1213 (ms / ep), 2.0972 MB reserved, 0.1490 MB allocated\n",
      "\n",
      "hidden_dim: 512, batch_size: 32\n",
      "Custom SyncBN: 1.3390 (ms / ep), 2.0972 MB reserved, 0.2243 MB allocated\n",
      "Torch SyncBN: 1.1263 (ms / ep), 2.0972 MB reserved, 0.1633 MB allocated\n",
      "\n",
      "hidden_dim: 512, batch_size: 64\n",
      "Custom SyncBN: 1.4554 (ms / ep), 2.0972 MB reserved, 0.4209 MB allocated\n",
      "Torch SyncBN: 1.3040 (ms / ep), 2.0972 MB reserved, 0.2944 MB allocated\n",
      "\n",
      "hidden_dim: 1024, batch_size: 32\n",
      "Custom SyncBN: 1.4569 (ms / ep), 2.0972 MB reserved, 0.4475 MB allocated\n",
      "Torch SyncBN: 1.2604 (ms / ep), 2.0972 MB reserved, 0.3231 MB allocated\n",
      "\n",
      "hidden_dim: 1024, batch_size: 64\n",
      "Custom SyncBN: 1.7162 (ms / ep), 2.0972 MB reserved, 0.8407 MB allocated\n",
      "Torch SyncBN: 1.5224 (ms / ep), 2.0972 MB reserved, 0.5852 MB allocated\n",
      "\n",
      "Rank 1\n",
      "hidden_dim: 128, batch_size: 32\n",
      "Custom SyncBN: 1.2775 (ms / ep), 2.0972 MB reserved, 0.0563 MB allocated\n",
      "Torch SyncBN: 1.0521 (ms / ep), 2.0972 MB reserved, 0.0563 MB allocated\n",
      "\n",
      "hidden_dim: 128, batch_size: 64\n",
      "Custom SyncBN: 1.2710 (ms / ep), 2.0972 MB reserved, 0.1055 MB allocated\n",
      "Torch SyncBN: 1.0691 (ms / ep), 2.0972 MB reserved, 0.1055 MB allocated\n",
      "\n",
      "hidden_dim: 256, batch_size: 32\n",
      "Custom SyncBN: 1.2805 (ms / ep), 2.0972 MB reserved, 0.1121 MB allocated\n",
      "Torch SyncBN: 1.0738 (ms / ep), 2.0972 MB reserved, 0.1121 MB allocated\n",
      "\n",
      "hidden_dim: 256, batch_size: 64\n",
      "Custom SyncBN: 1.3670 (ms / ep), 2.0972 MB reserved, 0.2104 MB allocated\n",
      "Torch SyncBN: 1.1232 (ms / ep), 2.0972 MB reserved, 0.2104 MB allocated\n",
      "\n",
      "hidden_dim: 512, batch_size: 32\n",
      "Custom SyncBN: 1.3429 (ms / ep), 2.0972 MB reserved, 0.2237 MB allocated\n",
      "Torch SyncBN: 1.1275 (ms / ep), 2.0972 MB reserved, 0.2237 MB allocated\n",
      "\n",
      "hidden_dim: 512, batch_size: 64\n",
      "Custom SyncBN: 1.4596 (ms / ep), 2.0972 MB reserved, 0.4204 MB allocated\n",
      "Torch SyncBN: 1.3050 (ms / ep), 2.0972 MB reserved, 0.4204 MB allocated\n",
      "\n",
      "hidden_dim: 1024, batch_size: 32\n",
      "Custom SyncBN: 1.4609 (ms / ep), 2.0972 MB reserved, 0.4470 MB allocated\n",
      "Torch SyncBN: 1.2619 (ms / ep), 2.0972 MB reserved, 0.4470 MB allocated\n",
      "\n",
      "hidden_dim: 1024, batch_size: 64\n",
      "Custom SyncBN: 1.7197 (ms / ep), 2.0972 MB reserved, 0.8402 MB allocated\n",
      "Torch SyncBN: 1.5228 (ms / ep), 2.0972 MB reserved, 0.8402 MB allocated\n",
      "\n",
      "[rank0]:[W223 14:28:42.834723054 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5 uv run torchrun --nproc_per_node 2 syncbn_benchmark.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week04_env",
   "language": "python",
   "name": "week04_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
